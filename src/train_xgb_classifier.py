# src/train_xgb_classifier.py
"""
Train an XGBoost classifier on the synthetic data generated by
`generate_synthetic_data.py`. The script loads the Excel file from the
`data/output` folder, performs minimal preprocessing (one‑hot encoding for
categorical columns and label‑encoding the target), splits the data, trains
an `XGBClassifier`, and prints the validation accuracy.

Requirements
------------
- pandas
- scikit‑learn
- xgboost

Usage
-----
```bash
python src/train_xgb_classifier.py
```
"""

import logging
import os
from datetime import datetime

# Configure logging
log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "training.log")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

import pandas as pd
import numpy as np
import joblib
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report
)
try:
    import shap
    import matplotlib.pyplot as plt
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not available. Install with: pip install shap matplotlib")

# ---------------------------------------------------------------------------
# Load synthetic data
# ---------------------------------------------------------------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
data_path = os.path.join(project_root, "data", "output", "synthetic_deals.xlsx")
if not os.path.exists(data_path):
    raise FileNotFoundError(f"Synthetic data not found at {data_path}")

df = pd.read_excel(data_path)

# ---------------------------------------------------------------------------
# Basic preprocessing
# ---------------------------------------------------------------------------
# Target: "Deal Status" (values: Won, Lost, Aborted)
if "Deal Status" not in df.columns:
    raise KeyError("Column 'Deal Status' not found in the dataset")

target = df["Deal Status"]
features = df.drop(columns=["Deal Status"])

# Encode target labels (Won=1, Lost/Aborted=0 for binary, or keep multi-class)
le = LabelEncoder()
y = le.fit_transform(target)

# Drop identifier and long-text columns from features
drop_cols = ["CRM ID", "Opportunity Name", "Account Name", "Detailed Remarks"]
X = df.drop(columns=[c for c in drop_cols if c in df.columns] + ["Deal Status"], errors="ignore").copy()

# 5. Basic preprocessing: detect numeric vs categorical
numeric_cols = X.select_dtypes(include=["int64","float64"]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]

# Clean categorical strings
for c in categorical_cols:
    X[c] = X[c].astype(str).str.strip().replace({"nan": np.nan, "None": np.nan})

# Convert numeric-like columns to numeric
for c in numeric_cols:
    X[c] = pd.to_numeric(X[c], errors="coerce")

# Impute missing values (simple)
if numeric_cols:
    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())
if categorical_cols:
    X[categorical_cols] = X[categorical_cols].fillna("UNKNOWN")

# 6. Encoding strategy: OneHot for low-cardinality, Ordinal for high-cardinality
onehot_feats = [c for c in categorical_cols if X[c].nunique() <= 10]
ordinal_feats = [c for c in categorical_cols if c not in onehot_feats]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_cols),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False), onehot_feats),
        ("ord", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1), ordinal_feats),
    ],
    remainder="drop",
    sparse_threshold=0
)

# 7. Train/test split (stratify to keep class balance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 8. XGBoost classifier pipeline
# Use multi:softprob for multi-class (Won/Lost/Aborted) or binary:logistic for binary
num_classes = len(np.unique(y))
if num_classes == 2:
    objective = "binary:logistic"
    eval_metric = "logloss"
else:
    objective = "multi:softprob"
    eval_metric = "mlogloss"

xgb_clf = xgb.XGBClassifier(
    objective=objective, 
    eval_metric=eval_metric, 
    n_jobs=-1, 
    random_state=42,
    n_estimators=1000  # Increased from 500
)

pipeline = Pipeline([
    ("prep", preprocessor),
    ("model", xgb_clf)
])

# 9. Baseline fit/Train Model
pipeline.fit(X_train, y_train)
preds = pipeline.predict(X_test)
probs = pipeline.predict_proba(X_test)

logger.info("\nBaseline classification metrics:")
logger.info(f"Accuracy : {accuracy_score(y_test, preds)}")
logger.info(f"Precision: {precision_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"Recall   : {recall_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"F1-score : {f1_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"\nConfusion Matrix:\n{confusion_matrix(y_test, preds)}")
logger.info(f"\nClassification Report:\n{classification_report(y_test, preds, target_names=le.classes_, zero_division=0)}")


# 10. Optional tuning (can be slow). Set do_tuning=True to run.
do_tuning = True
if do_tuning:
    param_dist = {
        "model__n_estimators": [500, 1000, 1500],  # Increased range
        "model__max_depth": [3, 5, 7, 9],
        "model__learning_rate": [0.01, 0.05, 0.1, 0.2],
        "model__subsample": [0.6, 0.8, 1.0],
        "model__colsample_bytree": [0.5, 0.7, 0.9, 1.0],
        "model__reg_alpha": [0, 0.1, 0.5, 1],
        "model__reg_lambda": [1, 5, 10, 20],
        "model__min_child_weight": [1, 3, 5]
    }
    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_dist,
        n_iter=20,
        cv=3,
        scoring="f1",
        verbose=2,
        random_state=42,
        n_jobs=-1
    )
    search.fit(X_train, y_train)
    print("Best params:", search.best_params_)
    best_model = search.best_estimator_
else:
    best_model = pipeline

# 11. Final evaluation
preds_best = best_model.predict(X_test)
print("\nFinal classification metrics:")
print("Accuracy :", accuracy_score(y_test, preds_best))
print("Precision:", precision_score(y_test, preds_best, average='weighted', zero_division=0))
print("Recall   :", recall_score(y_test, preds_best, average='weighted', zero_division=0))
print("F1-score :", f1_score(y_test, preds_best, average='weighted', zero_division=0))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, preds_best))
print("\nClassification Report:\n", classification_report(y_test, preds_best, target_names=le.classes_, zero_division=0))


# 12. SHAP explainability (small sample)
if SHAP_AVAILABLE:
    try:
        num_explain = min(50, X_test.shape[0])
        X_sample = X_test.sample(num_explain, random_state=42)
        X_transformed = best_model.named_steps["prep"].transform(X_sample)
        booster = best_model.named_steps["model"].get_booster()
        explainer = shap.TreeExplainer(booster)
        shap_values = explainer.shap_values(xgb.DMatrix(X_transformed))
        plt.figure(figsize=(8,6))
        shap.summary_plot(shap_values, X_transformed, show=False)
        plt.title("SHAP summary (sample)")
        plt.tight_layout()
        shap_png = os.path.join(project_root, "data", "output", "shap_summary_classifier.png")
        plt.savefig(shap_png, dpi=150)
        plt.close()
        print(f"Saved SHAP summary to {shap_png}")
    except Exception as e:
        print("SHAP plot skipped or failed:", str(e))
else:
    print("SHAP visualization skipped (library not installed)")

# 13. Save the trained pipeline
model_path = os.path.join(project_root, "models", "xgb_classifier.pkl")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(best_model, model_path)

# Also save the label encoder for later use
encoder_path = os.path.join(project_root, "models", "label_encoder.pkl")
joblib.dump(le, encoder_path)

print(f"\nModel saved to: {model_path}")
print(f"Label encoder saved to: {encoder_path}")
print(f"Classes: {le.classes_}")


