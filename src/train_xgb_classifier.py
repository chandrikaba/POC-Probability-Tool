# src/train_xgb_classifier.py
"""
Train an XGBoost classifier on the synthetic data generated by
`generate_synthetic_data.py`. The script loads the Excel file from the
`data/output` folder, performs minimal preprocessing (one‑hot encoding for
categorical columns and label‑encoding the target), splits the data, trains
an `XGBClassifier`, and prints the validation accuracy.

Requirements
------------
- pandas
- scikit‑learn
- xgboost

Usage
-----
```bash
python src/train_xgb_classifier.py
```
"""

import logging
import os
from datetime import datetime

# Configure logging
log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "training.log")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

import pandas as pd
import numpy as np
import joblib
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report
)
try:
    import shap
    import matplotlib.pyplot as plt
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not available. Install with: pip install shap matplotlib")

# ---------------------------------------------------------------------------
# Load synthetic data
# ---------------------------------------------------------------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
# Load Data
# Load Data
output_dir = os.path.join(project_root, "data", "output")
# Find all synthetic_data files
files = [f for f in os.listdir(output_dir) if f.startswith("synthetic_data") and f.endswith(".xlsx") and not f.startswith("~$")] # Ignore temp lock files
# Sort by modification time (latest first)
files.sort(key=lambda x: os.path.getmtime(os.path.join(output_dir, x)), reverse=True)

if not files:
    raise FileNotFoundError("No synthetic data found")
    
data_path = os.path.join(output_dir, files[0])
print(f"Loading latest data from: {data_path}")
    
if not os.path.exists(data_path):
    raise FileNotFoundError(f"Synthetic data not found at {data_path}")

df = pd.read_excel(data_path)

# ---------------------------------------------------------------------------
# Basic preprocessing
# ---------------------------------------------------------------------------
# Target: "Deal Status" (values: Won, Lost, Aborted)
if "Deal Status" not in df.columns:
    raise KeyError("Column 'Deal Status' not found in the dataset")

target = df["Deal Status"]
features = df.drop(columns=["Deal Status"])

# Encode target labels (Won=1, Lost/Aborted=0 for binary, or keep multi-class)
le = LabelEncoder()
y = le.fit_transform(target)

# Drop identifier and long-text columns from features
# Drop identifier and long-text columns from features
drop_cols = [
    "CRM ID", "Opportunity Name", "Account Name", "Detailed Remarks", 
    "Calculated Score", # Outcome variable (leakage)
    "Primary L1", "Primary L2", "Secondary L1", "Secondary L2", "Tertiary L1", "Tertiary L2" # Explanatory variables (leakage or unavailable at input)
]
X = df.drop(columns=[c for c in drop_cols if c in df.columns] + ["Deal Status"], errors="ignore").copy()

# 5. Basic preprocessing: detect numeric vs categorical
numeric_cols = X.select_dtypes(include=["int64","float64"]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]

# Clean categorical strings
for c in categorical_cols:
    X[c] = X[c].astype(str).str.strip().replace({"nan": np.nan, "None": np.nan})

# --- BUSINESS LOGIC: Explicit Ordinal Mapping ---
# Map categorical values to numbers so the model understands "High" > "Low"
ordinal_mappings = {
    "Account Engagement": {"High (Existing+Good)": 5, "Medium (Existing+Poor)": 3, "Low (New Account)": 0},
    "Client Relationship": {"Strong": 5, "Neutral": 3, "Weak": 0},
    "Deal Coach": {"Active & Available": 5, "Passive": 3, "Not Available": 0},
    "Bidder Rank": {"Top": 5, "Middle": 3, "Bottom": 0},
    "Incumbency Share": {"High (>50%)": 5, "Medium (20-50%)": 3, "Low (<20%)": 0, "None": 0},
    "References": {"Strong (Domain+Tech)": 5, "Average": 3, "Weak/None": 0},
    "Solution Strength": {"Strong (Covers all)": 5, "Average (Gaps)": 3, "Weak": 0},
    "Client Impression": {"Positive": 5, "Neutral": 3, "Negative": 0},
    "Orals Score": {"Strong": 5, "At Par": 3, "Weak": 0},
    "Price Alignment": {"Aligned": 5, "Deviating": 0, "No Intel": 2},
    "Price Position": {"Lowest": 5, "Competitive": 3, "Expensive": 0}
}

# Apply mappings
for col, mapping in ordinal_mappings.items():
    if col in X.columns:
        # Map values, fill unknown with 2 (Neutral-ish, better than Weak)
        X[col] = X[col].map(mapping).fillna(2)
        # Move from categorical to numeric list
        if col in categorical_cols:
            categorical_cols.remove(col)
        if col not in numeric_cols:
            numeric_cols.append(col)

# Convert numeric-like columns to numeric
for c in numeric_cols:
    X[c] = pd.to_numeric(X[c], errors="coerce")

# Impute missing values (simple)
if numeric_cols:
    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())
if categorical_cols:
    X[categorical_cols] = X[categorical_cols].fillna("UNKNOWN")

# 6. Encoding strategy: OneHot for remaining categorical cols
onehot_feats = [c for c in categorical_cols] # All remaining categoricals
# No ordinal feats left (we handled them manually)
ordinal_feats = []

preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_cols),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False), onehot_feats),
    ],
    remainder="drop",
    sparse_threshold=0
)

# 7. Train/test split (stratify to keep class balance)
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# 8. XGBoost classifier pipeline
# Use multi:softprob for multi-class (Won/Lost/Aborted) or binary:logistic for binary
num_classes = len(np.unique(y))
if num_classes == 2:
    objective = "binary:logistic"
    eval_metric = "logloss"
else:
    objective = "multi:softprob"
    eval_metric = "mlogloss"

xgb_clf = xgb.XGBClassifier(
    objective=objective, 
    eval_metric=eval_metric, 
    n_jobs=-1, 
    random_state=42,
    n_estimators=1000,
    early_stopping_rounds=50
)



pipeline = Pipeline([
    ("prep", preprocessor),
    ("model", xgb_clf)
])



# 9. Baseline fit/Train Model
#pipeline.fit(X_train, y_train)


pipeline.named_steps["model"].fit(
    preprocessor.fit_transform(X_train), y_train,
    eval_set=[(preprocessor.transform(X_test), y_test)],
    verbose=True
)

# for train_idx, val_idx in cv.split(X, y):
#     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
#     y_tr, y_val = y[train_idx], y[val_idx]
#     
#     pipeline.named_steps["model"].fit(
#         preprocessor.fit_transform(X_tr), y_tr,
#         eval_set=[(preprocessor.transform(X_val), y_val)],
#         early_stopping_rounds=50,
#         verbose=False
#     )
#     preds = pipeline.predict(X_val)
#     print("Fold F1:", f1_score(y_val, preds, average="weighted"))


preds = pipeline.predict(X_test)
probs = pipeline.predict_proba(X_test)

logger.info("\nBaseline classification metrics:")
logger.info(f"Accuracy : {accuracy_score(y_test, preds)}")
logger.info(f"Precision: {precision_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"Recall   : {recall_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"F1-score : {f1_score(y_test, preds, average='weighted', zero_division=0)}")
logger.info(f"\nConfusion Matrix:\n{confusion_matrix(y_test, preds)}")
logger.info(f"\nClassification Report:\n{classification_report(y_test, preds, target_names=le.classes_, zero_division=0)}")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Disable early stopping for CV as we don't pass eval_set
pipeline.named_steps["model"].set_params(early_stopping_rounds=None)

scores = cross_val_score(pipeline, X, y, cv=cv, scoring="f1_weighted")

print("Cross-validation F1 scores:", scores)
print("Mean F1:", scores.mean())

# 10. Optional tuning (can be slow). Set do_tuning=True to run.
do_tuning = True
if do_tuning:
    # Disable early stopping for randomized search (requires eval_set)
    pipeline.named_steps["model"].set_params(early_stopping_rounds=None)

    param_dist = {
        "model__n_estimators": [500, 1000, 1500],  # Increased range
        "model__max_depth": [3, 5, 7, 9],
        "model__learning_rate": [0.01, 0.05, 0.1, 0.2],
        "model__subsample": [0.6, 0.8, 1.0],
        "model__colsample_bytree": [0.5, 0.7, 0.9, 1.0],
        "model__reg_alpha": [0, 0.1, 0.5, 1],
        "model__reg_lambda": [1, 5, 10, 20],
        "model__min_child_weight": [1, 3, 5]
    }
    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_dist,
        n_iter=20,
        cv=3,
        scoring="f1",
        verbose=2,
        random_state=42,
        n_jobs=-1
    )
    search.fit(X_train, y_train)
    print("Best params:", search.best_params_)
    best_model = search.best_estimator_
else:
    best_model = pipeline

# 11. Final evaluation
preds_best = best_model.predict(X_test)
print("\nFinal classification metrics:")
print("Accuracy :", accuracy_score(y_test, preds_best))
print("Precision:", precision_score(y_test, preds_best, average='weighted', zero_division=0))
print("Recall   :", recall_score(y_test, preds_best, average='weighted', zero_division=0))
print("F1-score :", f1_score(y_test, preds_best, average='weighted', zero_division=0))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, preds_best))
print("\nClassification Report:\n", classification_report(y_test, preds_best, target_names=le.classes_, zero_division=0))


# 12. SHAP explainability (small sample)
if SHAP_AVAILABLE:
    try:
        num_explain = min(50, X_test.shape[0])
        X_sample = X_test.sample(num_explain, random_state=42)
        X_transformed = best_model.named_steps["prep"].transform(X_sample)
        booster = best_model.named_steps["model"].get_booster()
        explainer = shap.TreeExplainer(booster)
        shap_values = explainer.shap_values(xgb.DMatrix(X_transformed))
        plt.figure(figsize=(8,6))
        shap.summary_plot(shap_values, X_transformed, show=False)
        plt.title("SHAP summary (sample)")
        plt.tight_layout()
        shap_png = os.path.join(project_root, "data", "output", "shap_summary_classifier.png")
        plt.savefig(shap_png, dpi=150)
        plt.close()
        print(f"Saved SHAP summary to {shap_png}")
    except Exception as e:
        print("SHAP plot skipped or failed:", str(e))
else:
    print("SHAP visualization skipped (library not installed)")

# 13. Save the trained pipeline
model_path = os.path.join(project_root, "models", "xgb_classifier.pkl")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(best_model, model_path)

# Also save the label encoder for later use
encoder_path = os.path.join(project_root, "models", "label_encoder.pkl")
joblib.dump(le, encoder_path)

print(f"\nModel saved to: {model_path}")
print(f"Label encoder saved to: {encoder_path}")
print(f"Classes: {le.classes_}")


